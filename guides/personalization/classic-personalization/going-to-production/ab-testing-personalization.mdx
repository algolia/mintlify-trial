---
title: A/B testing Personalization
description: Learn how to A/B test Personalization.
---

import Beta from '/snippets/personalization/beta-migrate.mdx';

<Beta />

It's best to A/B test your Personalization implementation before going live with it to all your users.
This guide assumes knowledge of [Algolia's A/B testing feature](https://www.algolia.com/doc/guides/ab-testing/what-is-ab-testing/) and walks you through how to set up A/B tests for Personalization specifically.

Since you can save one Personalization strategy per application at a time, you can't test different Personalization strategies against each other
 You can A/B test non-personalized search versus personalized search or different levels of [Personalization impact](/guides/personalization/classic-personalization/configuring-personalization/#defining-personalization-impact).

If you're already sending events and including the `userToken` parameter with your searches, **you can A/B test Personalization entirely through the dashboard.**
For more information, see [Create and run and A/B test](https://www.algolia.com/doc/guides/ab-testing/what-is-ab-testing/how-to/create-and-run-an-ab-test/). 

## Testing non-personalized versus personalized results

Once you've [gathered user data](/guides/personalization/classic-personalization/personalizing-results/#capturing-user-behavior-by-sending-events) and
[configured your Personalization strategy](/guides/personalization/classic-personalization/configuring-personalization), it's time to test personalized against non-personalized results.

Since you can enable Personalization at query time, you don't need to create a [replica index](/guides/managing-results/refine-results/sorting/in-depth/replicas) for this A/B test.

<Note>
    A/B testing Personalization isn't a replacement for [simulating Personalization](/guides/personalization/classic-personalization/configuring-personalization/#simulating-personalization) while configuring your strategy.
    You should always simulate various users as part of your configuration process.
</Note>
 
### Using the dashboard

<Steps>
  <Step title="Turn off Personalization for the A variant">
    Ensure that **Enable Personalization** is off for the test index in the dashboard's [**Indices** section](https://dashboard.algolia.com/explorer/browse/).
    You can find this and other settings under **Configuration**.
  </Step>
  <Step title="Set Personalization impact for the B variant">
    In the dashboard's [**Personalization** section](https://dashboard.algolia.com/personalization/strategy), set the Personalization impact to a non-zero value.
    This ensures that when you enable Personalization for variant B, it has an effect.
  </Step>
  <Step title="Create an A/B test">
    Go to the dashboard's [**A/B testing** section](https://dashboard.algolia.com/ab-tests/overview).
    Click **New test** in the top left.
  </Step>
  <Step title="Name the test">
    Name your A/B test—something like "Non-personalized versus personalized results".
  </Step>
  <Step title="Select an index for each variant">
    Name and select your variant indices.
    Name **Variant A** "Non-personalized" and **Variant B** "Personalized."
    Select the same index for both.
  </Step>
  <Step title="Enable Personalization for variant B">
    Click **+ Add query parameter** under **Variant B**.
    In the box that appears, select the **Personalization tab**.
    Set **Enable Personalization** and **Tested value** to **on**.
  </Step>
  <Step title="Set the percentage of traffic for variant B">
    Select the percentage of traffic to send to variant B (the personalized version).
    The more confident you are in your Personalization implementation, the higher value you can use.
    If you want to expose fewer users to Personalization, select a lower value.
    The more uneven the traffic split, the longer the test could take to reach [statistical significance](https://www.algolia.com/doc/guides/ab-testing/what-is-ab-testing/in-depth/how-ab-test-scores-are-calculated/#statistical-significance-or-chance).
  </Step>
  <Step title="Determine test duration">
    Select the test duration.
    You should use at least two full business cycles.
    For example, if you see a predictable conversion trend over a week-long period, you should set the test to last at least two weeks.
    A shorter period might not include seasonality effects and end with biased results.
    Though not recommended, you can always stop a test early.
  </Step>
  <Step title="Save test">
    Click **Create** on the bottom right.
  </Step>
  <Step title="Verify test">
    From the [**A/B testing** section](https://dashboard.algolia.com/ab-tests/overview), verify that you've set up your test as planned:
    
    - Both variants use the same index.
    - Variant B displays **enablePersonalization:true** underneath the index's name.
    - The test is **Running** for the desired time.
    - The **Traffic split** between the variants is as planned.

    If the setup isn't correct, select **Delete A/B Test** and begin again.
  </Step>
</Steps>

Once the test is complete (and throughout the test if you wish), [interpret the results](https://www.algolia.com/doc/guides/ab-testing/what-is-ab-testing/how-to/create-and-run-an-ab-test/#interpreting-results).

## Testing different Personalization impacts

Suppose you've already established that Personalization is creating a better experience for your users and helping you meet your business goals.
In that case, you may want to fine-tune the Personalization impact—that is, the strength Personalization plays in your ranking formula.

For example, you can create an A/B test with different impacts:

- Variant A with an impact of 50
- Variant B with an impact = 70.

<Note>
    Don't run mopre than one A/B tests on the same index at the same time.
    If you want to test both Personalization in general and different impact scores using the same index: complete one test before starting the next.
</Note>
